---
title: "Lab2"
author: "Martynas Lukosevicius, Alejo Perez Gomez, Zahra Jalil Pour"
date: "10/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

### 1
```{r}
interpolator <- function(a, x){
 # stopifnot(x >= 0, x <=1 )
  X <- as.matrix(c(1, x, x^2), ncol = 1 )
  return(as.vector(a %*% X))
}

SSE <- function(x, a, method1, method2){
  real <- sapply(x, method1)
  pred <- sapply(x, method2, a = a)
  return(sum((real-pred)^2))
}

optimiser <- function(x, method){
  aInit <- rep(1,3)
  res <- optim(aInit, SSE, x = x, method1 = method, method2 = interpolator)
  return(res$par)
}
```

### 2 
```{r}
aproximate <- function(n, method){
  scale <- 1/n
  midVal <- scale / 2
  result <- list()
  for (i in 1:n) {
    x <- c((scale*i)-scale, (scale * i) - midVal, scale * i)
    result <- append(result, list(optimiser(x, method)))
  }
  return(result)
}
```

### 3
```{r}
f1 <- function(x){
  return(-x * (1-x))
}

f2 <- function(x){
  return(-x * sin(10 * pi * x))
}


x <- seq(0, 1, by = 0.01)
plot(x,f1(x))

test <- aproximate(100, f1)
for (a in test) {
  yint <- sapply(x, interpolator, a = a)
  lines(x,yint)
}
```

```{r}

f2 <- function(x){
  return(-x * sin(10 * pi * x))
}

x <- seq(0, 1, by = 0.01)
plot(x,f2(x))

test <- aproximate(100, f2)
for (a in test) {
  yint <- sapply(x, interpolator, a = a)
  lines(x,yint)
}
```

## Question 2
### 1
```{r}
load("data.RData")
```

### 2
$$y \sim N(\mu, \sigma^2)$$
$$L(p(\mu, \sigma^2|y)) = \prod^n_{i = 1} \frac 1 {\sqrt{2 \pi \sigma^2} } e^ - { \frac {(y - \mu)^2}{2 \sigma^2}} = \frac 1 {(\sqrt{2 \pi \sigma^2})^n } e^ - \sum^n_{i = 1} \frac{(y_i - \mu)^2} {2\sigma^2} $$
$$ln L(p(\mu, \sigma|y)) = - \frac n {2} ln(2 \pi \sigma^2) - \sum^n_{i = 1} \frac{(y_i - \mu)^2} {2\sigma^2}$$
$$ \frac{\partial lnL(p(\mu, \sigma|y))} {\partial \mu} = -\frac{1}{2\sigma^2} \frac{\partial(\sum y_i^2-2\mu \sum y_i + n \mu^2)}{\partial \mu} = -\frac{1}{2\sigma^2} (0 - 2\sum y_i + 2n\mu) = \frac{\sum y_i - n\mu}{\sigma^2} $$

$$MLE => \frac{\sum y_i - n\mu}{\sigma^2} = 0$$
$$\hat \mu_{MLE} = \frac{\sum y_i}{n} $$
$$ \frac{\partial lnL(p(\mu, \sigma|y))} {\partial \sigma} = - \frac{n}{\sigma} + \frac{1}{\sigma^3} \sum^n_{i=1}(y_i - \mu)^2 $$
$$ MLE => - \frac{n}{\sigma} + \frac{1}{\sigma^3} \sum^n_{i=1}(y_i - \mu)^2 = 0$$
$$\hat{\sigma}^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (y_i- \mu)^2$$
```{r}
 meanEst <- mean(data)
print(meanEst)

sqrt((sum((data-meanEst)^2))/ length(data))
```
### 3

$$ln L(p(\mu, \sigma|y)) = - \frac n {2} ln(2 \pi \sigma^2) - \sum^n_{i = 1} \frac{(y_i - \mu)^2} {2\sigma^2}$$

```{r}


minusLogLikelihood <- function(x, n, data){
  mu <- x[1]
  sigma <- x[2]
  part1 <-  (n/2) * log(2*pi * (sigma^2))
  part2 <- (sum(data - mu)^2) / (2* sigma^2)
  return(part1 + part2)
}



init <- c(0, 1)
test1 <- optim(init, minusLogLikelihood, n = 100, data = data,  method = "CG", control=list(fnscale= -1))
test2 <- optim(init, minusLogLikelihood, n = 100, data = data, method = "BFGS", control=list(fnscale= -1))

```

