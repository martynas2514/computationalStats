---
title: "Computer Lab 6"
author: "Martynas Lukosevicius, Alejo Perez Gomez, Zahra Jalilpour"
date: '2020-12-09'
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, error = F, warning = F,
                      fig.align='center', out.width="70%")
knitr::opts_chunk$set(cache = TRUE)
```

# Question 1: Genetic algorithm

## 1. 

Function 
$$f(x) = \frac {x^2} {e^x} - 2 exp(-(9sinx)/(x^2 + x +1))$$
```{r, echo = TRUE}
f <- function(x){
  part1 <- (x^2) / exp(x)
  expPart <- -(9*sin(x)) / ((x^2) + x + 1)
  return(part1 -2 * exp(expPart))
}
```
## 2.
Crosover function $\frac {x+y} {2}$:

```{r, echo = TRUE}
crossover <- function(x,y){
  return((x+y)/2)
}
```
## 3.

mutate function $x^2 mod 30$:

```{r, echo = TRUE}
mutate <- function(x){
  return((x^2)%% 30)
}
```
## 4. 

Function implementing genetic algorithm:

 1. initialazise parameters
 2. select parents
 3. select victim
 4. generate kid by crosover and mutation
 5. replace victim with kid 
 6. save best value
 7. repeat steps 2 to 3 maxiterations times.
 
```{r, echo = FALSE}
genetic <- function(maxiter,mutprob){
  # list for task 5 
  res <- list(initial = numeric(), final = numeric())
  
  # a)
  x <- seq(0,30, 0.1)
  plot(x,f(x), type = "l", main = paste("maxiter:", maxiter,"mutprob:", mutprob))
  
  # b)
  X <- seq(0,30,5)
  res$initial <- X
  # c)
  
  Values <- f(X)
  
  # d)
  
 for (i in 1:maxiter) {
   # i.
   parents <- sample(X,2)
   # ii.
   victim <- order(Values)[1]
   # iii.
   kid <- crossover(parents[1], parents[2])
   if(runif(1) < mutprob){
     kid <- mutate(kid)
   }
   # iv.
   X[victim] <- kid
   Values <- f(X)
   # v.
   maxX <- X[which.max(Values)]
   #points(maxX, f(maxX), col="blue", pch ="x")
 }
  res$final <- X
  points(maxX, f(maxX), col="red", pch ="x")
  return(res)
}
```

## 5. 

```{r,echo=FALSE, error=FALSE, warning=FALSE}
par(mfrow=c(2,3))
 m10m1 <- genetic(10,0.1)
 m10m5 <- genetic(10,0.5)
 m10m9 <- genetic(10,0.9)
 
 m100m1 <- genetic(100,0.1)
 m100m5 <- genetic(100,0.5)
 m100m9 <- genetic(100,0.9)
 
 res <- rbind(m10m1$initial, 
       m10m1$final, 
       m10m5$final, 
       m10m9$final, 
       m100m1$final, 
       m100m5$final, 
       m100m9$final
       )
 
 rownames(res) <- c("initial",
                    "10 0.1",
                    "10 0.5",
                    "10 0.9",
                    "100 0.1",
                    "100 0.5",
                    "100 0.9"
                    )

 knitr::kable(res)
```
10 iterations are not enough for genetic algorithm to find the max value. It can be seen that increasing mutation probability, max value will fluctuate around the max value and will not reach it.

# Question 2: EM algorithm

## 1.
```{r}
df <- read.csv("physical1.csv")
```
physical data that we have analyzed, consists of two physical process $Y$ and $Z$.Here we make time series plot. Both variables start with high value and decrease by time. Variable $Z$ has eight missing value. Both series have similar pattern. 
```{r, echo = FALSE}
library(ggplot2)
palette_OkabeIto_black <- c("blue1", "maroon")
p<- ggplot(data=df)
p+geom_line(aes(X, Y, color="Z"))+
  geom_line(aes(X, Z, color="Y"))+
  scale_color_manual(values = palette_OkabeIto_black)+
  theme_bw()+
  labs(x="Time", y="variables Y , Z")+
  ggtitle("Dependencies of Y and Z versus X")

```

## 2.
As we see missing values in $Z$ series , we will have problem in estimating model by maximum likelihood.
Here we assume that we have these two different models for $Y$ and $Z$:

$$Y_i = exp(\frac{X_i}{\lambda})   , Z_i = exp(\frac{X_i}{2\lambda})$$

$\lambda$ is some unknown parameter.
The aim of this task is to derive an EM algorithm to estimate $\lambda$ .
Expected value of the log likelihood function of $\lambda$ can be defined as:
$$Q(\lambda, \lambda^k) = E[loglik(\lambda | X,Y,Z)|\lambda^k, X,Y,Z]$$

### pdf
Probability density function of an exponential distribution equals:
$$ X\sim Exp(\lambda)~=> pdf= \lambda e^{-\lambda x}$$
Here we assume $Y$ is observed data and $Z$ consists of latent data
$$ Y_i \sim Exp(\frac{X_i}{\lambda}) ~=> pdf = \frac{X_i}{\lambda} e^{-\frac{X_i}{\lambda}Y_i}$$
$$ Z_i \sim Exp(\frac{X_i}{2 \lambda}) ~=> pdf = \frac{X_i}{2 \lambda} e^{-\frac{X_i}{2 \lambda}Z_i} $$

### likelihood
$${l}(\lambda|Y,Z) =\prod_{i=1}^{n} p(Y_i, Z_i | \lambda)= \prod_{i=1}^{n}p(Y_i |\lambda) p(Z_i |\lambda)=\prod_{i=1}^{n}[\frac{X_i}{\lambda} e^{-\frac{X_i}{\lambda}Y_i}].\prod_{i=1}^{n}[\frac{X_i}{2 \lambda} e^{-\frac{X_i}{2 \lambda}Z_i}]$$

### Log_likelihood
$$ln({l}(\lambda|Y,Z)) = ln(\prod_{i=1}^{n}[\frac{X_i}{\lambda} e^{-\frac{X_i}{\lambda}Y_i}]. \prod_{i=1}^{n}[\frac{X_i}{2 \lambda} e^{-\frac{X_i}{2 \lambda}Z_i}])$$

$$ln({l}(\lambda|Y,Z))= ln(\sum x_i^2)- nln(2)-2nln(\lambda)- \frac {\sum X_i Y_i}{\lambda} - \frac{\sum^{obs} X_i Z_i}{2 \lambda} - \frac {\sum^{not obs} X_i Z_i}{2\lambda}$$
In this equation we have not considered missing values in $Z$. As $Z$ consists of missing values, it is not possible to calculate likelihood of this model. Hence we divide $Z$ into two series , observed and unobserved values, $Z=(Z^o, Z^u)$ and compute Expected log_likelihood. We assume $Z^o$ by length r and $Z^u$ by length n-r.



### Max_log_likelihood Estimation:
In this step, to find the maximum likelihood , we should derive the expected log_likelihood according to $\lambda$ and set the equation to zero to calculate $\lambda$. We consider $\lambda^k$ as unobserved parameter. It is M_step:
$$Q(\lambda, \lambda^k)= ln(\sum x_i^2)- nln(2)-2nln(\lambda)- \frac {\sum X_i Y_i}{\lambda} - \frac{\sum^{obs} X_i Z_i}{2 \lambda} - \frac {j \lambda_j}{\lambda} $$
Now we should derive this function according to $\lambda$ and set to zero.
$$ \frac{\partial{E} [L(\lambda)] }{\partial{\lambda}} = -\dfrac{4n \lambda-\sum^{n} X_i Y_i-2\sum^{obs} X_i Z_i -2j\lambda_j}{2\lambda^2}$$
$$\lambda = \dfrac{\sum^{obs} X_i Z_i+2\sum^{n} X_i Y_i+2j\lambda_j}{4n}$$

## 3.
```{r echo = TRUE}
 em_exp <- function(data, eps, kmax,lambda0){
  # observed Z and X associated to it
   obs <- data[!is.na(data$Z), ]
   miss <- data[is.na(data$Z), ]
    n <- length(data$X)
    r <- length(miss$X)
    # initialize iteration
    llvalcurr <- lambda0
    k <- 0
    llvalprev <- 0
    while((abs(llvalprev-llvalcurr)>eps) && (k<(kmax+1))){
      llvalprev<-llvalcurr
      llvalcurr <- ((t(obs$X) %*% obs$Z) + 
                                 2*(t(df$X) %*% df$Y) +
                                 2 * length(miss$X) * llvalcurr)/(4*length(df$X))
       k <- k+1
    }
    return(list(iterations = k, lambda = llvalcurr))
    }

```


```{r}
knitr::kable(t(em_exp(df,0.001, 50, 100)), col.names = c("iteartions", "lambda"))
```
## 4. 
```{r}

expectedvalz = (2 * 10.69566) / df$X
expectedvaly =  10.69566 / df$X
df <- cbind(df,expectedvalz, expectedvaly, deparse.level = 1)
```

```{r, echo=FALSE}
palette_OkabeIto_black <- c("Z" = "blue1", "Y" = "maroon","E[z]" = "red","E[y]" = "black")
p<- ggplot(data=df)
p+geom_line(aes(X, Y , col = "Y"))+
  geom_line(aes(X, Z, col = "Z"))+
  geom_line(aes(X, expectedvalz , col = "E[z]"))+
  geom_line(aes(X, expectedvaly, col = "E[y]"))+
  scale_color_manual(values = palette_OkabeIto_black)+
  theme_bw()+
  labs(x="Time", y="variables Y , Z, E[Y], E[Z]") +
  ggtitle("Dependencies of Y , Z, E[Y], E[Z] versus X")
```


# Appendix

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```