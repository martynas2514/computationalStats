---
title: "Untitled"
author: "zahra jalilpour"
date: '2020-12-09'
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, error = F, warning = F,
                      fig.align='center', out.width="70%")
knitr::opts_chunk$set(cache = TRUE)
```
### task1
```{r}
df <- read.csv("physical1.csv")
```
physical data that we have analyzed, consists of two physical process $Y$ and $Z$.Here we make time series plot. Both variables start with high value and decrease by time. Variable $Z$ has eight missing value. Both series have similar pattern. 
```{r}
library(ggplot2)
palette_OkabeIto_black <- c("blue1", "maroon")
p<- ggplot(data=df)
p+geom_line(aes(X, Y, color="Z"))+
  geom_line(aes(X, Z, color="Y"))+
  scale_color_manual(values = palette_OkabeIto_black)+
  theme_bw()+
  labs(x="Time", y="variables Y , Z")
  ggtitle("Dependencies of Y and Z versus X")

```

### task 2
As we see missing values in $Z$ series , we will have problem in estimating model by maximum likelihood.
Here we assume that we have these two different models for $Y$ and $Z$:

$$Y_i = exp(\frac{X_i}{\lambda})   , Z_i = exp(\frac{X_i}{2\lambda})$$

$\lambda$ is some unknown parameter.
The aim of this task is to derive an EM algorithm to estimate $\lambda$ .
Expected value of the log likelihood function of $\lambda$ can be defined as:
$$Q(\lambda, \lambda^k) = E[loglik(\lambda | X,Y,Z)|\lambda^k, X,Y,Z]$$

#### pdf
Probability density function of an exponential distribution equals:
$$ X\sim Exp(\lambda)~=> pdf= \lambda e^{-\lambda x}$$
Here we assume $Y$ is observed data and $Z$ consists of latent data
$$ Y_i \sim Exp(\frac{X_i}{\lambda}) ~=> pdf = \frac{X_i}{\lambda} e^{-\frac{X_i}{\lambda}Y_i}$$
$$ Z_i \sim Exp(\frac{X_i}{2 \lambda}) ~=> pdf = \frac{X_i}{2 \lambda} e^{-\frac{X_i}{2 \lambda}Z_i} $$

#### likelihood
$${l}(\lambda|Y,Z) =\prod_{i=1}^{n} p(Y_i, Z_i | \lambda)= \prod_{i=1}^{n}p(Y_i |\lambda) p(Z_i |\lambda)=\prod_{i=1}^{n}[\frac{X_i}{\lambda} e^{-\frac{X_i}{\lambda}Y_i}].\prod_{i=1}^{n}[\frac{X_i}{2 \lambda} e^{-\frac{X_i}{2 \lambda}Z_i}]$$

#### Log_likelihood
$$ln({l}(\lambda|Y,Z)) = ln(\prod_{i=1}^{n}[\frac{X_i}{\lambda} e^{-\frac{X_i}{\lambda}Y_i}]. \prod_{i=1}^{n}[\frac{X_i}{2 \lambda} e^{-\frac{X_i}{2 \lambda}Z_i}])$$

$$ln({l}(\lambda|Y,Z))= ln(\sum x_i^2)- nln(2)-2nln(\lambda)- \frac {\sum X_i Y_i}{\lambda} - \frac{\sum^{obs} X_i Z_i}{2 \lambda} - \frac {\sum^{not obs} X_i Z_i}{2\lambda}$$
In this equation we have not considered missing values in $Z$. As $Z$ consists of missing values, it is not possible to calculate likelihood of this model. Hence we divide $Z$ into two series , observed and unobserved values, $Z=(Z^o, Z^u)$ and compute Expected log_likelihood. We assume $Z^o$ by length r and $Z^u$ by length n-r.



#### Max_log_likelihood Estimation:
In this step, to find the maximum likelihood , we should derive the expected log_likelihood according to $\lambda$ and set the equation to zero to calculate $\lambda$. We consider $\lambda^k$ as unobserved parameter. It is M_step:
$$Q(\lambda, \lambda^k)= ln(\sum x_i^2)- nln(2)-2nln(\lambda)- \frac {\sum X_i Y_i}{\lambda} - \frac{\sum^{obs} X_i Z_i}{2 \lambda} - \frac {j \lambda_j}{\lambda} $$
Now we should derive this function according to $\lambda$ and set to zero.
$$ \frac{\partial{E} [L(\lambda)] }{\partial{\lambda}} = -\dfrac{4n \lambda-\sum^{n} X_i Y_i-2\sum^{obs} X_i Z_i -2j\lambda_j}{2\lambda^2}$$
$$\lambda = \dfrac{\sum^{obs} X_i Z_i+2\sum^{n} X_i Y_i+2j\lambda_j}{4n}$$

### task3
```{r}
 em_exp <- function(data, eps, kmax,lambda0){
  # observed Z and X associated to it
   obs <- data[!is.na(data$Z), ]
   miss <- data[is.na(data$Z), ]
    n <- length(data$X)
    r <- length(miss$X)
    # initialize iteration
    llvalcurr <- lambda0
    k <- 0
    llvalprev <- 0
    #paste(c(k,llvalprev,llvalcurr))
    while((abs(llvalprev-llvalcurr)>eps) && (k<(kmax+1))){
      llvalprev<-llvalcurr
      ## E-step
      llvalcurr <- ((t(obs$X) %*% obs$Z) + 
                                 2*(t(df$X) %*% df$Y) +
                                 2 * length(miss$X) * llvalcurr)/(4*length(df$X))
       k <- k+1
    }
    return(c(k,llvalprev,llvalcurr))
    }

```


